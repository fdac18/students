{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adesai6', 'akarnauc', 'awilki13', 'bbass11', 'cflemmon', 'cjohn3', 'cmawhinn', 'dbarry', 'eezell3', 'eherron5', 'gjones2', 'gyj992', 'hchang13', 'jdong6', 'jdunca51', 'jpace7', 'jpovlin', 'jyu25', 'lpassare', 'mander59', 'mkramer6', 'mmahbub', 'mousavi', 'nmansou4', 'nschwerz', 'pgoedec1', 'pprovins', 'rdabbs1', 'rhossai2', 'ssadhu2', 'ssteinb2', 'trahman4', 'zrandall']\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "import os\n",
    "os.system(\"ls [a-z]*.md | sed 's/\\.md$//'|grep -v ports| sort -u >ids.txt\")\n",
    "with open('ids.txt') as f:\n",
    "    files = [ x.strip('\\n') for x in f.readlines() ]\n",
    "print (files)\n",
    "\n",
    "# Create labels for the graph\n",
    "flab = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "documents = []\n",
    "\n",
    "text = \"\"\n",
    "for f in files:\n",
    "  a = open (f+\".md\")\n",
    "  line = a.read()\n",
    "  documents .append(line)\n",
    "  text = text + \" \" + line\n",
    "      \n",
    "#Do tf.idf magic \n",
    "tfv = TfidfVectorizer()\n",
    "tfidf = tfv.fit_transform(documents)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "# we may want to see these, e.g., analyze in R\n",
    "pd .DataFrame.from_records(tfidf.A).to_csv('tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adesai6</th>\n",
       "      <th>akarnauc</th>\n",
       "      <th>awilki13</th>\n",
       "      <th>bbass11</th>\n",
       "      <th>cflemmon</th>\n",
       "      <th>cjohn3</th>\n",
       "      <th>cmawhinn</th>\n",
       "      <th>dbarry</th>\n",
       "      <th>eezell3</th>\n",
       "      <th>eherron5</th>\n",
       "      <th>...</th>\n",
       "      <th>nmansou4</th>\n",
       "      <th>nschwerz</th>\n",
       "      <th>pgoedec1</th>\n",
       "      <th>pprovins</th>\n",
       "      <th>rdabbs1</th>\n",
       "      <th>rhossai2</th>\n",
       "      <th>ssadhu2</th>\n",
       "      <th>ssteinb2</th>\n",
       "      <th>trahman4</th>\n",
       "      <th>zrandall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adesai6</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>akarnauc</th>\n",
       "      <td>0.050</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awilki13</th>\n",
       "      <td>0.082</td>\n",
       "      <td>0.216</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbass11</th>\n",
       "      <td>0.118</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cflemmon</th>\n",
       "      <td>0.142</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cjohn3</th>\n",
       "      <td>0.074</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.153</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cmawhinn</th>\n",
       "      <td>0.110</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.235</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dbarry</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eezell3</th>\n",
       "      <td>0.072</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.140</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eherron5</th>\n",
       "      <td>0.096</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.211</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gjones2</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gyj992</th>\n",
       "      <td>0.120</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hchang13</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jdong6</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jdunca51</th>\n",
       "      <td>0.055</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jpace7</th>\n",
       "      <td>0.076</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jpovlin</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jyu25</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lpassare</th>\n",
       "      <td>0.079</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mander59</th>\n",
       "      <td>0.173</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mkramer6</th>\n",
       "      <td>0.084</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmahbub</th>\n",
       "      <td>0.062</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mousavi</th>\n",
       "      <td>0.093</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nmansou4</th>\n",
       "      <td>0.114</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nschwerz</th>\n",
       "      <td>0.142</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgoedec1</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.156</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pprovins</th>\n",
       "      <td>0.113</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rdabbs1</th>\n",
       "      <td>0.052</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rhossai2</th>\n",
       "      <td>0.056</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssadhu2</th>\n",
       "      <td>0.118</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.083</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ssteinb2</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trahman4</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.202</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zrandall</th>\n",
       "      <td>0.128</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.185</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      adesai6             akarnauc             awilki13  \\\n",
       "adesai6                 1.000                0.050                0.082   \n",
       "akarnauc                0.050                1.000                0.216   \n",
       "awilki13                0.082                0.216                1.000   \n",
       "bbass11                 0.118                0.172                0.185   \n",
       "cflemmon                0.142                0.162                0.190   \n",
       "cjohn3                  0.074                0.126                0.226   \n",
       "cmawhinn                0.110                0.061                0.116   \n",
       "dbarry                  0.063                0.059                0.126   \n",
       "eezell3                 0.072                0.095                0.149   \n",
       "eherron5                0.096                0.185                0.239   \n",
       "gjones2                 0.058                0.185                0.194   \n",
       "gyj992                  0.120                0.195                0.202   \n",
       "hchang13                0.047                0.168                0.252   \n",
       "jdong6                  0.136                0.118                0.196   \n",
       "jdunca51                0.055                0.104                0.125   \n",
       "jpace7                  0.076                0.079                0.125   \n",
       "jpovlin                 0.052                0.046                0.081   \n",
       "jyu25                   0.100                0.289                0.233   \n",
       "lpassare                0.079                0.087                0.143   \n",
       "mander59                0.173                0.308                0.192   \n",
       "mkramer6                0.084                0.239                0.313   \n",
       "mmahbub                 0.062                0.127                0.179   \n",
       "mousavi                 0.093                0.079                0.122   \n",
       "nmansou4                0.114                0.239                0.271   \n",
       "nschwerz                0.142                0.224                0.247   \n",
       "pgoedec1                0.094                0.099                0.165   \n",
       "pprovins                0.113                0.098                0.123   \n",
       "rdabbs1                 0.052                0.157                0.242   \n",
       "rhossai2                0.056                0.114                0.154   \n",
       "ssadhu2                 0.118                0.072                0.101   \n",
       "ssteinb2                0.136                0.167                0.340   \n",
       "trahman4                0.047                0.107                0.201   \n",
       "zrandall                0.128                0.200                0.207   \n",
       "\n",
       "                      bbass11             cflemmon               cjohn3  \\\n",
       "adesai6                 0.118                0.142                0.074   \n",
       "akarnauc                0.172                0.162                0.126   \n",
       "awilki13                0.185                0.190                0.226   \n",
       "bbass11                 1.000                0.175                0.246   \n",
       "cflemmon                0.175                1.000                0.153   \n",
       "cjohn3                  0.246                0.153                1.000   \n",
       "cmawhinn                0.168                0.064                0.235   \n",
       "dbarry                  0.200                0.105                0.227   \n",
       "eezell3                 0.204                0.085                0.258   \n",
       "eherron5                0.276                0.180                0.232   \n",
       "gjones2                 0.152                0.087                0.189   \n",
       "gyj992                  0.154                0.217                0.119   \n",
       "hchang13                0.121                0.104                0.109   \n",
       "jdong6                  0.191                0.171                0.130   \n",
       "jdunca51                0.088                0.076                0.070   \n",
       "jpace7                  0.137                0.095                0.130   \n",
       "jpovlin                 0.098                0.157                0.074   \n",
       "jyu25                   0.142                0.206                0.160   \n",
       "lpassare                0.241                0.141                0.214   \n",
       "mander59                0.158                0.245                0.101   \n",
       "mkramer6                0.206                0.196                0.248   \n",
       "mmahbub                 0.242                0.131                0.176   \n",
       "mousavi                 0.082                0.125                0.070   \n",
       "nmansou4                0.155                0.289                0.188   \n",
       "nschwerz                0.172                0.226                0.132   \n",
       "pgoedec1                0.170                0.207                0.202   \n",
       "pprovins                0.127                0.089                0.096   \n",
       "rdabbs1                 0.285                0.155                0.244   \n",
       "rhossai2                0.248                0.139                0.214   \n",
       "ssadhu2                 0.087                0.120                0.099   \n",
       "ssteinb2                0.169                0.163                0.116   \n",
       "trahman4                0.170                0.207                0.203   \n",
       "zrandall                0.238                0.191                0.118   \n",
       "\n",
       "                     cmawhinn               dbarry              eezell3  \\\n",
       "adesai6                 0.110                0.063                0.072   \n",
       "akarnauc                0.061                0.059                0.095   \n",
       "awilki13                0.116                0.126                0.149   \n",
       "bbass11                 0.168                0.200                0.204   \n",
       "cflemmon                0.064                0.105                0.085   \n",
       "cjohn3                  0.235                0.227                0.258   \n",
       "cmawhinn                1.000                0.133                0.153   \n",
       "dbarry                  0.133                1.000                0.140   \n",
       "eezell3                 0.153                0.140                1.000   \n",
       "eherron5                0.107                0.155                0.211   \n",
       "gjones2                 0.151                0.082                0.161   \n",
       "gyj992                  0.043                0.081                0.091   \n",
       "hchang13                0.053                0.105                0.043   \n",
       "jdong6                  0.059                0.168                0.120   \n",
       "jdunca51                0.071                0.056                0.084   \n",
       "jpace7                  0.068                0.086                0.102   \n",
       "jpovlin                 0.135                0.112                0.025   \n",
       "jyu25                   0.122                0.208                0.078   \n",
       "lpassare                0.161                0.145                0.212   \n",
       "mander59                0.063                0.107                0.085   \n",
       "mkramer6                0.156                0.188                0.125   \n",
       "mmahbub                 0.090                0.231                0.177   \n",
       "mousavi                 0.032                0.058                0.125   \n",
       "nmansou4                0.120                0.109                0.095   \n",
       "nschwerz                0.046                0.139                0.081   \n",
       "pgoedec1                0.048                0.119                0.114   \n",
       "pprovins                0.142                0.084                0.065   \n",
       "rdabbs1                 0.197                0.209                0.200   \n",
       "rhossai2                0.144                0.223                0.110   \n",
       "ssadhu2                 0.099                0.056                0.104   \n",
       "ssteinb2                0.052                0.103                0.105   \n",
       "trahman4                0.078                0.151                0.153   \n",
       "zrandall                0.055                0.120                0.104   \n",
       "\n",
       "                     eherron5         ...                      nmansou4  \\\n",
       "adesai6                 0.096         ...                         0.114   \n",
       "akarnauc                0.185         ...                         0.239   \n",
       "awilki13                0.239         ...                         0.271   \n",
       "bbass11                 0.276         ...                         0.155   \n",
       "cflemmon                0.180         ...                         0.289   \n",
       "cjohn3                  0.232         ...                         0.188   \n",
       "cmawhinn                0.107         ...                         0.120   \n",
       "dbarry                  0.155         ...                         0.109   \n",
       "eezell3                 0.211         ...                         0.095   \n",
       "eherron5                1.000         ...                         0.176   \n",
       "gjones2                 0.207         ...                         0.207   \n",
       "gyj992                  0.183         ...                         0.205   \n",
       "hchang13                0.199         ...                         0.149   \n",
       "jdong6                  0.161         ...                         0.187   \n",
       "jdunca51                0.118         ...                         0.112   \n",
       "jpace7                  0.090         ...                         0.196   \n",
       "jpovlin                 0.061         ...                         0.157   \n",
       "jyu25                   0.143         ...                         0.615   \n",
       "lpassare                0.194         ...                         0.115   \n",
       "mander59                0.131         ...                         0.313   \n",
       "mkramer6                0.175         ...                         0.219   \n",
       "mmahbub                 0.200         ...                         0.103   \n",
       "mousavi                 0.070         ...                         0.092   \n",
       "nmansou4                0.176         ...                         1.000   \n",
       "nschwerz                0.144         ...                         0.288   \n",
       "pgoedec1                0.183         ...                         0.143   \n",
       "pprovins                0.060         ...                         0.103   \n",
       "rdabbs1                 0.214         ...                         0.102   \n",
       "rhossai2                0.155         ...                         0.202   \n",
       "ssadhu2                 0.095         ...                         0.070   \n",
       "ssteinb2                0.242         ...                         0.165   \n",
       "trahman4                0.202         ...                         0.175   \n",
       "zrandall                0.177         ...                         0.232   \n",
       "\n",
       "                     nschwerz             pgoedec1             pprovins  \\\n",
       "adesai6                 0.142                0.094                0.113   \n",
       "akarnauc                0.224                0.099                0.098   \n",
       "awilki13                0.247                0.165                0.123   \n",
       "bbass11                 0.172                0.170                0.127   \n",
       "cflemmon                0.226                0.207                0.089   \n",
       "cjohn3                  0.132                0.202                0.096   \n",
       "cmawhinn                0.046                0.048                0.142   \n",
       "dbarry                  0.139                0.119                0.084   \n",
       "eezell3                 0.081                0.114                0.065   \n",
       "eherron5                0.144                0.183                0.060   \n",
       "gjones2                 0.087                0.127                0.112   \n",
       "gyj992                  0.116                0.155                0.118   \n",
       "hchang13                0.133                0.120                0.081   \n",
       "jdong6                  0.154                0.156                0.106   \n",
       "jdunca51                0.072                0.073                0.083   \n",
       "jpace7                  0.053                0.123                0.113   \n",
       "jpovlin                 0.059                0.038                0.209   \n",
       "jyu25                   0.301                0.139                0.097   \n",
       "lpassare                0.044                0.139                0.099   \n",
       "mander59                0.211                0.146                0.167   \n",
       "mkramer6                0.220                0.116                0.124   \n",
       "mmahbub                 0.161                0.123                0.076   \n",
       "mousavi                 0.064                0.083                0.148   \n",
       "nmansou4                0.288                0.143                0.103   \n",
       "nschwerz                1.000                0.156                0.077   \n",
       "pgoedec1                0.156                1.000                0.061   \n",
       "pprovins                0.077                0.061                1.000   \n",
       "rdabbs1                 0.096                0.176                0.113   \n",
       "rhossai2                0.141                0.135                0.094   \n",
       "ssadhu2                 0.036                0.084                0.100   \n",
       "ssteinb2                0.132                0.162                0.131   \n",
       "trahman4                0.123                0.156                0.038   \n",
       "zrandall                0.300                0.146                0.162   \n",
       "\n",
       "                      rdabbs1             rhossai2              ssadhu2  \\\n",
       "adesai6                 0.052                0.056                0.118   \n",
       "akarnauc                0.157                0.114                0.072   \n",
       "awilki13                0.242                0.154                0.101   \n",
       "bbass11                 0.285                0.248                0.087   \n",
       "cflemmon                0.155                0.139                0.120   \n",
       "cjohn3                  0.244                0.214                0.099   \n",
       "cmawhinn                0.197                0.144                0.099   \n",
       "dbarry                  0.209                0.223                0.056   \n",
       "eezell3                 0.200                0.110                0.104   \n",
       "eherron5                0.214                0.155                0.095   \n",
       "gjones2                 0.159                0.203                0.106   \n",
       "gyj992                  0.132                0.123                0.061   \n",
       "hchang13                0.154                0.102                0.018   \n",
       "jdong6                  0.137                0.152                0.046   \n",
       "jdunca51                0.083                0.069                0.017   \n",
       "jpace7                  0.097                0.092                0.065   \n",
       "jpovlin                 0.119                0.120                0.013   \n",
       "jyu25                   0.113                0.188                0.042   \n",
       "lpassare                0.212                0.115                0.153   \n",
       "mander59                0.078                0.127                0.051   \n",
       "mkramer6                0.224                0.160                0.044   \n",
       "mmahbub                 0.233                0.141                0.073   \n",
       "mousavi                 0.071                0.072                0.027   \n",
       "nmansou4                0.102                0.202                0.070   \n",
       "nschwerz                0.096                0.141                0.036   \n",
       "pgoedec1                0.176                0.135                0.084   \n",
       "pprovins                0.113                0.094                0.100   \n",
       "rdabbs1                 1.000                0.185                0.100   \n",
       "rhossai2                0.185                1.000                0.083   \n",
       "ssadhu2                 0.100                0.083                1.000   \n",
       "ssteinb2                0.161                0.122                0.061   \n",
       "trahman4                0.223                0.213                0.033   \n",
       "zrandall                0.121                0.163                0.062   \n",
       "\n",
       "                     ssteinb2             trahman4             zrandall  \n",
       "adesai6                 0.136                0.047                0.128  \n",
       "akarnauc                0.167                0.107                0.200  \n",
       "awilki13                0.340                0.201                0.207  \n",
       "bbass11                 0.169                0.170                0.238  \n",
       "cflemmon                0.163                0.207                0.191  \n",
       "cjohn3                  0.116                0.203                0.118  \n",
       "cmawhinn                0.052                0.078                0.055  \n",
       "dbarry                  0.103                0.151                0.120  \n",
       "eezell3                 0.105                0.153                0.104  \n",
       "eherron5                0.242                0.202                0.177  \n",
       "gjones2                 0.139                0.091                0.165  \n",
       "gyj992                  0.257                0.145                0.190  \n",
       "hchang13                0.301                0.161                0.209  \n",
       "jdong6                  0.345                0.201                0.245  \n",
       "jdunca51                0.172                0.051                0.132  \n",
       "jpace7                  0.196                0.078                0.155  \n",
       "jpovlin                 0.059                0.126                0.091  \n",
       "jyu25                   0.164                0.149                0.213  \n",
       "lpassare                0.095                0.068                0.107  \n",
       "mander59                0.285                0.097                0.434  \n",
       "mkramer6                0.156                0.159                0.239  \n",
       "mmahbub                 0.152                0.187                0.117  \n",
       "mousavi                 0.158                0.092                0.116  \n",
       "nmansou4                0.165                0.175                0.232  \n",
       "nschwerz                0.132                0.123                0.300  \n",
       "pgoedec1                0.162                0.156                0.146  \n",
       "pprovins                0.131                0.038                0.162  \n",
       "rdabbs1                 0.161                0.223                0.121  \n",
       "rhossai2                0.122                0.213                0.163  \n",
       "ssadhu2                 0.061                0.033                0.062  \n",
       "ssteinb2                1.000                0.202                0.276  \n",
       "trahman4                0.202                1.000                0.185  \n",
       "zrandall                0.276                0.185                1.000  \n",
       "\n",
       "[33 rows x 33 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the similarity matrix\n",
    "# first create labels\n",
    "idx = {}\n",
    "for i in range(len(files)):\n",
    "    idx[i] = files [i]\n",
    "\n",
    "# Multiply the tfidf matrix to get all pairwise distances    \n",
    "df = pd. DataFrame.from_records((tfidf * tfidf.T).A,columns=files)\n",
    "\n",
    "#do pretty printing\n",
    "pd.options.display.float_format = '{:20,.3f}'.format\n",
    "df = df.rename(index=idx)\n",
    "#Output to a file in case we want to look at it separately\n",
    "df.to_csv(\"dist.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mkramer6', 'zrandall', 'lpassare', 'rhossai2', 'ssadhu2', 'hchang13', 'nschwerz', 'mander59', 'bbass11', 'akarnauc', 'gyj992', 'nmansou4', 'jpovlin', 'mmahbub', 'rdabbs1', 'adesai6', 'ssteinb2', 'cmawhinn', 'cjohn3', 'eherron5', 'jpace7', 'jdunca51', 'dbarry', 'jyu25', 'gjones2', 'eezell3', 'awilki13', 'jdong6', 'pprovins', 'cflemmon', 'mousavi', 'pgoedec1', 'trahman4']\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "# use these distances to create a graph by cutting links below .21 \n",
    "nxg = nx.Graph()\n",
    "for i in range(len(files)-1):\n",
    "    nLinks = 0\n",
    "    maxVal = 0\n",
    "    jMax = 0\n",
    "    for j in range(i+1,len(files)):\n",
    "        if df.iloc[i,j] > maxVal: \n",
    "            maxVal = df.iloc[i,j]\n",
    "            jMax = j\n",
    "        if (df.iloc[i,j] > .37):\n",
    "            nLinks += 1\n",
    "            nxg.add_edge(flab[i], flab[j], weight=df.iloc[i,j],color='r')\n",
    "    if nLinks == 0:\n",
    "        nxg.add_edge(flab[i], flab[jMax], weight=df.iloc[i,jMax],color='r')\n",
    "        \n",
    "print (nxg.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAKFCAYAAABVxzqvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73ccd2e5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import linkage, ward, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Calculate cosine distance (clustering needs distance, not similarity)   \n",
    "#dist ij = dist2 [i*(n-1)+j] and i < j\n",
    "dist1 = pdist(tfidf.A, 'cosine')\n",
    "minVal = min(dist1)\n",
    "ij = np.where(dist1==minVal)[0][0]\n",
    "for i in range(0,32):\n",
    "    for j in range (i+1,33):\n",
    "        ij -= 1\n",
    "        if ij == 0: \n",
    "            break\n",
    "    else:        \n",
    "        continue  \n",
    "    break\n",
    "#print (flab[i] + \" \" + flab[j+1] + \" \"+ str (minVal))\n",
    "#a = open (flab[i]+\".md\")\n",
    "#line = a.read()\n",
    "#print(line)\n",
    "#a = open (flab[j+1]+\".md\")\n",
    "#line = a.read()\n",
    "#print(line)\n",
    "\n",
    "\n",
    "# Do clustering\n",
    "linkage_matrix = linkage (dist1, method='complete')\n",
    "#print(linkage_matrix)\n",
    "#print (flab[int(linkage_matrix[0][0])] + \" \" + flab[int(linkage_matrix[0][1])])\n",
    "# Plot results\n",
    "fig = plt.figure(1, figsize=(8, 11))\n",
    "plt.clf()\n",
    "ddata = dendrogram(linkage_matrix,\n",
    "               color_threshold=1,\n",
    "               p=97,\n",
    "               labels=flab,\n",
    "               truncate_mode='none',\n",
    "               orientation='left',\n",
    "                show_leaf_counts=True,\n",
    "               )\n",
    "plt.title(\"Clustering of Descriptions\")\n",
    "fig.savefig(\"teaming.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "nld = json_graph.node_link_data(nxg)\n",
    "json.dump(nld, open('students.json','w'))\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import display\n",
    "# IPython Notebook can serve files and display them into\n",
    "# inline frames. Prepend the path with the 'files' prefix.\n",
    "viz_file = 'files/students.html'\n",
    "display(IFrame(viz_file, '100%', '400px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Approaches for text analysis\n",
    "\n",
    "## BOW + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mander59 [ 0.12255168] I am Matt Anderson and I am a senior majoring in Computer Science with a minor in Business. I am interested in cyber security and looking forward to this class.\n",
      "\n",
      "mkramer6 [ 0.14413479] I am Matthew Kramer and I am a senior currently pursuing a Computer Science Degree with a minor in Cyber Security. I instantly found a love for programming after my first computer science class in High School and am trying to convert that into a stable occupation. I chose this class due to the description sounding interesting and being outside of my comfort zone. I hope to learn a lot in this class to better gauge what I want to work on when I complete my degree. When I am not working on my computer, I love to rock climb and play guitar.\n",
      "\n",
      "zrandall [ 0.12020136] I am Zach Randall and I am a senior in Computer Engineering. I am interested in cyber security and am excited to some data analysis skills from this class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tfv_lookup = { word: idx for idx, word in enumerate(tfv.get_feature_names())}\n",
    "#print(tfidf.get_shape)\n",
    "srch = tfv_lookup['computer']\n",
    "dst = tfidf[:,srch].A\n",
    "#find a person closest to the term 'computer'\n",
    "for i in range(len(flab)): \n",
    "  if dst[i]>.11: print (flab[i] +' '+ str(dst[i]) + ' ' + documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI - latent semantic indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import LsiModel\n",
    "from gensim.corpora import Dictionary, TextCorpus\n",
    "\n",
    "\n",
    "f = open(\"corpus.txt\",\"w\") \n",
    "for i in range(len(flab)):\n",
    "  line = re.sub('\\n', ' ', documents[i])\n",
    "  f.write(line+'\\n') \n",
    "f.close()\n",
    "\n",
    "corpus = TextCorpus(\"corpus.txt\")\n",
    "\n",
    "model = LsiModel (corpus,num_topics=10)\n",
    "\n",
    "vectorized_corpus = model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities of index corpus documents to one another:\n",
      "Similarities of index corpus documents to 'computer science'\n",
      "array([ 0.83143246,  0.60598791,  0.71987462,  0.59611022,  0.68048453,\n",
      "        0.38446194,  0.58106637,  0.29453352,  0.45433268,  0.65690213,\n",
      "        0.57344204,  0.41936022,  0.57131064,  0.8539145 ,  0.67521656,\n",
      "        0.38126767,  0.53018165,  0.66558719,  0.69742972,  0.59654331,\n",
      "        0.55147201,  0.25590408,  0.8567552 ,  0.63078189,  0.78591979,\n",
      "        0.64524204,  0.63960844,  0.57123148,  0.52445459,  0.18217914,\n",
      "        0.79378128,  0.34792608,  0.76985043], dtype=float32)\n",
      "The document most similar to the query is 'I am Sara Mousavi majoring in computer science. I am intersted in deep learning, data analytics, programming, playing phone games, watching TV and working out.\n",
      "' with a score of 0.86.\n"
     ]
    }
   ],
   "source": [
    "from gensim.similarities import Similarity\n",
    "import pprint\n",
    "from gensim.corpora import WikiCorpus, wikicorpus\n",
    "tokenize_func = wikicorpus.tokenize \n",
    "\n",
    "sdocs = [\"computer science\",\n",
    "             \"data science\" ]\n",
    "# A corpus can be anything, as long as iterating over it produces a representation of the corpus documents as vectors.\n",
    "crp1 = (corpus.dictionary.doc2bow(tokenize_func(document)) for document in sdocs)\n",
    "\n",
    "index = Similarity(corpus=model[corpus], num_features=10, output_prefix=\"shard\")\n",
    "\n",
    "print (\"Similarities of index corpus documents to one another:\")\n",
    "#pprint.pprint([s for s in index])\n",
    "\n",
    "query = \"computer science\"\n",
    "sims_to_query = index[model[corpus.dictionary.doc2bow(tokenize_func(query))]]\n",
    "print (\"Similarities of index corpus documents to '%s'\" % query)\n",
    "pprint.pprint (sims_to_query)\n",
    "\n",
    "best_score = max(sims_to_query)\n",
    "idx = sims_to_query.tolist().index(best_score)\n",
    "most_similar_doc = documents [idx]\n",
    "print (\"The document most similar to the query is '%s' with a score of %.2f.\" % (most_similar_doc, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document most similar to 'data science' is 'I am Trish Goedecke; a consulting statistician at UTHSC in Memphis, and a student in UTK's data science and engineering program. I am eager to learn python, and looking to join projects applying cutting edge data science to medical research, in areas such as genetics, streaming data, or data mining of electronic medical records. \n",
      "' with a score of 0.92.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"data science\"\n",
    "sims_to_query1 = index[model[corpus.dictionary.doc2bow(tokenize_func(query1))]]\n",
    "best_score = max(sims_to_query1)\n",
    "idx = sims_to_query1.tolist().index(best_score)\n",
    "most_similar_doc = documents [idx]\n",
    "print (\"The document most similar to 'data science' is '%s' with a score of %.2f.\" % (most_similar_doc, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - Latent Dirichle allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.032*\"science\" + 0.017*\"data\" + 0.016*\"senior\" + 0.016*\"majoring\" + 0.014*\"class\" + 0.011*\"interested\" + 0.009*\"security\"'),\n",
       " (1,\n",
       "  '0.047*\"data\" + 0.029*\"science\" + 0.018*\"engineering\" + 0.015*\"student\" + 0.011*\"work\" + 0.010*\"phd\" + 0.009*\"like\"'),\n",
       " (2,\n",
       "  '0.029*\"data\" + 0.024*\"science\" + 0.020*\"class\" + 0.018*\"senior\" + 0.014*\"learn\" + 0.010*\"like\" + 0.008*\"looking\"')]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the corpus.\n",
    "from gensim.models import LdaModel\n",
    "lda = LdaModel(corpus, id2word=corpus.dictionary, num_topics=3)\n",
    "lda .show_topics(num_topics=3,num_words=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.18567717862476027), (1, 0.60891251395246992), (2, 0.20541030742276994)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.031*\"data\" + 0.029*\"science\" + 0.023*\"class\" + 0.020*\"senior\" + 0.014*\"forward\" + 0.012*\"looking\" + 0.009*\"student\" + 0.008*\"minor\" + 0.008*\"engineering\" + 0.008*\"security\"'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"computer science\"\n",
    "query = corpus.dictionary.doc2bow(tokenize_func(query))\n",
    "print(lda[query])\n",
    "lda.print_topic(lda[query][-1][0]) #prind second topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The second topic seems to fit this query the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import gzip\n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "import random\n",
    "class LabeledLineSentence(object):\n",
    "  def __init__(self, sources):\n",
    "    self.sources = sources\n",
    "        \n",
    "    flipped = {}\n",
    "        \n",
    "    # make sure that keys are unique\n",
    "    for key, value in sources.items():\n",
    "      if value not in flipped:\n",
    "        flipped[value] = [key]\n",
    "      else:\n",
    "        raise Exception('Non-unique prefix encountered')\n",
    "   \n",
    "  def __iter__(self):\n",
    "    for source, prefix in self.sources.items():\n",
    "      with gzip.open(source) as fin:\n",
    "        for item_no, line in enumerate(fin):          \n",
    "          yield LabeledSentence(utils.to_unicode(line).split(), [prefix ])\n",
    "    \n",
    "  def to_array(self):\n",
    "    self.sentences = []\n",
    "    for source, prefix in self.sources.items():\n",
    "      with gzip.open(source) as fin:\n",
    "        for item_no, line in enumerate(fin):\n",
    "          self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [ prefix ]))\n",
    "    return self.sentences\n",
    "    \n",
    "  def sentences_perm(self):\n",
    "    shuffled = list(self.sentences)\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "corpus = LabeledLineSentence(sources={'pos.gz':'POS', 'neg.gz':'NEG'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=5 w=3 acc=0.67594\n",
      "ep=10 w=3 acc=0.65458\n",
      "ep=15 w=3 acc=0.78578\n",
      "ep=20 w=3 acc=0.86942\n",
      "ep=5 w=5 acc=0.70572\n",
      "ep=10 w=5 acc=0.80946\n",
      "ep=15 w=5 acc=0.88344\n",
      "ep=20 w=5 acc=0.88484\n",
      "ep=5 w=10 acc=0.8531\n",
      "ep=10 w=10 acc=0.88816\n",
      "ep=15 w=10 acc=0.8847\n",
      "ep=20 w=10 acc=0.86502\n",
      "ep=5 w=15 acc=0.88116\n",
      "ep=10 w=15 acc=0.88032\n",
      "ep=15 w=15 acc=0.87614\n",
      "ep=20 w=15 acc=0.83662\n"
     ]
    }
   ],
   "source": [
    "import math;\n",
    "\n",
    "for w in (3, 5, 10, 15):\n",
    " for ep in (5, 10, 15, 20):\n",
    "  d2v = Doc2Vec(min_count=1, window=w, size=100, sample=1e-4, negative=5, workers=7)\n",
    "  d2v.build_vocab(corpus.to_array())\n",
    "  d2v.train(corpus.sentences_perm(),epochs=ep,total_examples=d2v.corpus_count)\n",
    "  d2v.save('./imdb.ep'+str(ep)+'.w'+str(w)+'.d2v') #10 is optimal 40 bad\n",
    "  nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "  np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "  pred = [];\n",
    "  for i in range(50000):\n",
    "   iv = d2v.infer_vector(corpus.sentences[i][0])\n",
    "   ni = math.sqrt(sum(iv*iv))\n",
    "   ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "   ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "   pred.append(ivp > ivn)\n",
    "  tru = 0;\n",
    "  for i in range(50000):\n",
    "   if pred[i] == True and i < 25000: tru += 1\n",
    "   if pred[i] == False and i >= 25000: tru += 1   \n",
    "  print ('ep='+str(ep)+' w='+str(w)+' acc='+str(tru/50000.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=25 w=3 acc=0.8832\n",
      "ep=40 w=3 acc=0.8723\n"
     ]
    }
   ],
   "source": [
    "for w in [3]:\n",
    " for ep in (25, 40):\n",
    "  d2v = Doc2Vec(min_count=1, window=w, size=100, sample=1e-4, negative=5, workers=7)\n",
    "  d2v.build_vocab(corpus.to_array())\n",
    "  d2v.train(corpus.sentences_perm(),epochs=ep,total_examples=d2v.corpus_count)\n",
    "  d2v.save('./imdb.ep'+str(ep)+'.w'+str(w)+'.d2v') #10 is optimal 40 bad\n",
    "  nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "  np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "  pred = [];\n",
    "  for i in range(50000):\n",
    "   iv = d2v.infer_vector(corpus.sentences[i][0])\n",
    "   ni = math.sqrt(sum(iv*iv))\n",
    "   ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "   ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "   pred.append(ivp > ivn)\n",
    "  tru = 0;\n",
    "  for i in range(50000):\n",
    "   if pred[i] == True and i < 25000: tru += 1\n",
    "   if pred[i] == False and i >= 25000: tru += 1   \n",
    "  print ('ep='+str(ep)+' w='+str(w)+' acc='+str(tru/50000.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=20 w=2 acc=0.79114\n",
      "ep=25 w=2 acc=0.85564\n",
      "ep=30 w=2 acc=0.87276\n",
      "ep=20 w=4 acc=0.88708\n",
      "ep=25 w=4 acc=0.8794\n",
      "ep=30 w=4 acc=0.87422\n"
     ]
    }
   ],
   "source": [
    "for w in [2,4]:\n",
    " for ep in (20, 25, 30):\n",
    "  d2v = Doc2Vec(min_count=1, window=w, size=100, sample=1e-4, negative=5, workers=7)\n",
    "  d2v.build_vocab(corpus.to_array())\n",
    "  d2v.train(corpus.sentences_perm(),epochs=ep,total_examples=d2v.corpus_count)\n",
    "  d2v.save('./imdb.ep'+str(ep)+'.w'+str(w)+'.d2v') #10 is optimal 40 bad\n",
    "  nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "  np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "  pred = [];\n",
    "  for i in range(50000):\n",
    "   iv = d2v.infer_vector(corpus.sentences[i][0])\n",
    "   ni = math.sqrt(sum(iv*iv))\n",
    "   ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "   ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "   pred.append(ivp > ivn)\n",
    "  tru = 0;\n",
    "  for i in range(50000):\n",
    "   if pred[i] == True and i < 25000: tru += 1\n",
    "   if pred[i] == False and i >= 25000: tru += 1   \n",
    "  print ('ep='+str(ep)+' w='+str(w)+' acc='+str(tru/50000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=9 w=9 acc=0.87768\n",
      "ep=10 w=9 acc=0.88528\n",
      "ep=11 w=9 acc=0.88566\n",
      "ep=9 w=11 acc=0.88646\n",
      "ep=10 w=11 acc=0.88618\n",
      "ep=11 w=11 acc=0.88326\n",
      "ep=9 w=10 acc=0.88324\n",
      "ep=10 w=10 acc=0.8873\n",
      "ep=11 w=10 acc=0.88654\n"
     ]
    }
   ],
   "source": [
    "for w in (9, 11, 10):\n",
    " for ep in (9, 10, 11):\n",
    "  d2v = Doc2Vec(min_count=1, window=w, size=100, sample=1e-4, negative=5, workers=7)\n",
    "  d2v.build_vocab(corpus.to_array())\n",
    "  d2v.train(corpus.sentences_perm(),epochs=ep,total_examples=d2v.corpus_count)\n",
    "  d2v.save('./imdb.ep'+str(ep)+'.w'+str(w)+'.d2v') #10 is optimal 40 bad\n",
    "  nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "  np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "  pred = [];\n",
    "  for i in range(50000):\n",
    "   iv = d2v.infer_vector(corpus.sentences[i][0])\n",
    "   ni = math.sqrt(sum(iv*iv))\n",
    "   ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "   ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "   pred.append(ivp > ivn)\n",
    "  tru = 0;\n",
    "  for i in range(50000):\n",
    "   if pred[i] == True and i < 25000: tru += 1\n",
    "   if pred[i] == False and i >= 25000: tru += 1   \n",
    "  print ('ep='+str(ep)+' w='+str(w)+' acc='+str(tru/50000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=10 w=10 sa=2e-05 ns=6 sz=90 acc=0.89238\n",
      "ep=10 w=10 sa=2e-05 ns=6 sz=95 acc=0.88904\n",
      "ep=10 w=10 sa=2e-05 ns=7 sz=90 acc=0.88722\n",
      "ep=10 w=10 sa=2e-05 ns=7 sz=95 acc=0.88878\n",
      "ep=10 w=10 sa=3e-05 ns=6 sz=90 acc=0.8862\n",
      "ep=10 w=10 sa=3e-05 ns=6 sz=95 acc=0.89096\n",
      "ep=10 w=10 sa=3e-05 ns=7 sz=90 acc=0.8929\n",
      "ep=10 w=10 sa=3e-05 ns=7 sz=95 acc=0.8901\n"
     ]
    }
   ],
   "source": [
    "w=10\n",
    "ep = 10\n",
    "for sa in (2e-5, 3e-5):\n",
    " for ns in [ 6, 7 ]:\n",
    "  for sz in (90, 95):   \n",
    "   d2v = Doc2Vec(min_count=1, window=w, size=sz, sample=sa, negative=ns, workers=7)\n",
    "   d2v.build_vocab(corpus.to_array())\n",
    "   d2v.train(corpus.sentences_perm(),epochs=ep,total_examples=d2v.corpus_count)\n",
    "   #d2v.save('./imdb.ep'+str(ep)+'.w'+str(w)+'.d2v') #10 is optimal 40 bad\n",
    "   nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "   np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "   pred = [];\n",
    "   for i in range(50000):\n",
    "    iv = d2v.infer_vector(corpus.sentences[i][0])\n",
    "    ni = math.sqrt(sum(iv*iv))\n",
    "    ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "    ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "    pred.append(ivp > ivn)\n",
    "   tru = 0;\n",
    "   for i in range(50000):\n",
    "    if pred[i] == True and i < 25000: tru += 1\n",
    "    if pred[i] == False and i >= 25000: tru += 1   \n",
    "   print ('ep='+str(ep)+' w='+str(w)+' sa='+str(sa)+' ns='+str(ns)+' sz='+str(sz)+' acc='+str(tru/50000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7006831765174866),\n",
       " ('decent', 0.6766880750656128),\n",
       " ('bad', 0.6580454111099243),\n",
       " ('nice', 0.6188746690750122),\n",
       " ('terrific', 0.5916984677314758),\n",
       " ('solid', 0.5897783041000366),\n",
       " ('fine', 0.5601633787155151),\n",
       " ('roselina', 0.5582941770553589),\n",
       " ('reak', 0.5578945279121399),\n",
       " ('passable', 0.5531889796257019)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.79838222,  0.12151772, -0.92435265, -0.46299124, -0.56452048,\n",
       "       -1.23210418, -2.44014001, -0.59871   ,  0.26792476, -0.23406811,\n",
       "        0.1818139 , -1.03144228, -3.09669399, -2.52545571, -2.1112473 ,\n",
       "       -3.09592795, -0.10257489,  3.07035899, -1.93702567,  0.34065786,\n",
       "       -0.12825036, -0.77809   ,  0.96191663,  4.0352335 ,  1.71774435,\n",
       "       -0.71466851,  5.20107794, -0.73876977,  1.27344465, -3.76774216,\n",
       "       -0.21790522, -1.31231022,  0.69909656, -0.81849337, -0.50536007,\n",
       "       -3.62961912, -2.72662163, -1.08575761, -4.18522501,  1.55017424,\n",
       "        2.31208134,  2.20991755,  1.70716846,  1.67720509,  1.56481862,\n",
       "        0.77370089, -1.66742194, -1.384179  , -1.51816308, -0.46514204,\n",
       "       -0.82033628, -1.93164146,  1.1580441 ,  3.42229342, -1.47762001,\n",
       "        0.42112908,  0.32617787, -0.61749125, -3.32173586, -2.56895471,\n",
       "        1.01622355,  0.57171178,  0.07857493, -3.90092397, -1.35944903,\n",
       "       -1.57938325, -3.28408742, -0.81211299,  0.24366033, -0.05362059,\n",
       "        2.58457899,  2.56753063, -3.64239216,  0.0640933 ,  2.01278925,\n",
       "        0.94937265, -0.76092666, -3.25167942,  0.27477315,  0.64116639,\n",
       "       -1.76767111,  1.07244241, -0.65324306,  0.40204442, -0.76475513,\n",
       "        2.02273536, -0.4523415 , -0.84853309,  1.83350301,  1.33142781,\n",
       "        0.53512758,  0.64308614, -0.31689632, -3.03221297,  0.14433858,\n",
       "       -2.27217579,  1.21233308, -0.34153467, -1.26188076,  0.72203583], dtype=float32)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v.docvecs['NEG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nschwerz sentiment:0.0244336601645\n",
      "trahman4 sentiment:0.0135693650126\n"
     ]
    }
   ],
   "source": [
    "import math;\n",
    "for i in range(len(flab)):\n",
    "  iv = d2v.infer_vector(documents[0].split())\n",
    "  ni = math.sqrt(sum(iv*iv))\n",
    "  nn = math.sqrt(sum(d2v.docvecs['NEG']*d2v.docvecs['NEG']))\n",
    "  np = math.sqrt(sum(d2v.docvecs['POS']*d2v.docvecs['POS']))\n",
    "  ivn = sum (iv * d2v.docvecs['NEG'])/ni/nn\n",
    "  ivp = sum (iv * d2v.docvecs['POS'])/ni/np\n",
    "  if ivp > ivn: print(flab[i]+' sentiment:'+str(ivp-ivn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### only two students have positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import gzip\n",
    "import numpy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "import random\n",
    "class LabeledLineSentence1(object):\n",
    "  def __init__(self, sources):\n",
    "    self.sources = sources\n",
    "        \n",
    "    flipped = {}\n",
    "        \n",
    "    # make sure that keys are unique\n",
    "    for key, value in sources.items():\n",
    "      if value not in flipped:\n",
    "        flipped[value] = [key]\n",
    "      else:\n",
    "        raise Exception('Non-unique prefix encountered')\n",
    "   \n",
    "  def __iter__(self):\n",
    "    for source, prefix in self.sources.items():\n",
    "      with gzip.open(source) as fin:\n",
    "        for item_no, line in enumerate(fin):          \n",
    "          yield LabeledSentence(utils.to_unicode(line).split(), [prefix, str(item_no) ])\n",
    "    \n",
    "  def to_array(self):\n",
    "    self.sentences = []\n",
    "    for source, prefix in self.sources.items():\n",
    "      with gzip.open(source) as fin:\n",
    "        for item_no, line in enumerate(fin):\n",
    "          self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [ prefix, str(item_no) ]))\n",
    "    return self.sentences\n",
    "    \n",
    "  def sentences_perm(self):\n",
    "    shuffled = list(self.sentences)\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled\n",
    "\n",
    "corpus1 = LabeledLineSentence1(sources={'pos.gz':'POS', 'neg.gz':'NEG'})\n",
    "\n",
    "d2v2 = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "d2v2.build_vocab(corpus1.to_array())\n",
    "d2v2.train(corpus1.sentences_perm(),epochs=20, total_examples=d2v.corpus_count)\n",
    "d2v2.save('./imdb.ep20d2v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = [];\n",
    "pred1 = [];\n",
    "import math;\n",
    "nn = math.sqrt(sum(d2v2.docvecs['NEG']*d2v2.docvecs['NEG']))\n",
    "np = math.sqrt(sum(d2v2.docvecs['POS']*d2v2.docvecs['POS']))\n",
    "\n",
    "for i in range(50000):\n",
    "  j = i;\n",
    "  if i >= 25000: j = i - 25000\n",
    "  iv = d2v2.infer_vector(corpus1.sentences[i][0])\n",
    "  iv1 = d2v2.docvecs [str(j)]\n",
    "  ni = math.sqrt(sum(iv*iv))\n",
    "  ni1 = math.sqrt(sum(iv1*iv1))\n",
    "  ivn = sum (iv * d2v2.docvecs['NEG'])/ni/nn\n",
    "  ivp = sum (iv * d2v2.docvecs['POS'])/ni/np\n",
    "  ivn1 = sum (iv1 * d2v2.docvecs['NEG'])/ni/nn\n",
    "  ivp1 = sum (iv1 * d2v2.docvecs['POS'])/ni/np\n",
    "  pred.append(ivp > ivn)\n",
    "  pred1.append(ivp1 > ivn1)\n",
    "\n",
    "tru = 0;\n",
    "tru1 = 0;\n",
    "for i in range(50000):\n",
    "  if pred[i] == True and i < 25000: tru += 1\n",
    "  if pred[i] == False and i >= 25000: tru += 1\n",
    "  if pred1[i] == True and i < 25000: tru1 += 1\n",
    "  if pred1[i] == False and i >= 25000: tru1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88132\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print (str(tru/50000.0))\n",
    "print (str(tru1/50000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
